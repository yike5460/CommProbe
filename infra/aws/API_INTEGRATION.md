# Reddit Crawler API Integration Guide

## Overview

The Reddit Crawler API provides REST endpoints for triggering and monitoring Reddit data collection jobs. This API is designed to be consumed by the Cloudflare Workers API Gateway.

## Architecture

The API is implemented as a dedicated AWS Lambda function (`/infra/aws/lambda/api/index.py`) that integrates with AWS Step Functions to orchestrate the Reddit crawling pipeline. This follows the same architecture pattern as other Lambda functions in the system.

## API Endpoints

### Base URL
After deployment, the API will be available at:
```
https://{api-id}.execute-api.{region}.amazonaws.com/v1/
```

### Authentication
The API uses API Key authentication. Include the API key in the `X-API-Key` header:
```
X-API-Key: your-api-key-here
```

## Endpoints

### 1. GET / - API Documentation
Get API information and available endpoints.

**Response:**
```json
{
  "service": "Supio Reddit Legal Communities Crawler API",
  "version": "1.0",
  "endpoints": {
    "POST /trigger": "Start a new crawl job",
    "GET /status/{executionName}": "Check execution status",
    "GET /executions": "List recent executions",
    "GET /": "This documentation"
  },
  "trigger_parameters": {
    "subreddits": "array of subreddit names (optional)",
    "crawl_type": "crawl, search, or both (optional)",
    "days_back": "number of days to look back (optional)"
  }
}
```

### 2. POST /trigger - Start New Crawl Job
Trigger a new Reddit crawling job with optional parameters.

**Request Body (Optional):**
```json
{
  "subreddits": ["LawFirm", "Lawyertalk", "legaltech"],
  "crawl_type": "both",
  "days_back": 3
}
```

**Response:**
```json
{
  "message": "Crawl job started successfully",
  "executionArn": "arn:aws:states:us-east-1:123456789:execution:supio-reddit-insights-pipeline:manual-20250101-120000-abc12345",
  "executionName": "manual-20250101-120000-abc12345",
  "startDate": "2025-01-01T12:00:00.000000",
  "parameters": {
    "manual_trigger": true,
    "trigger_time": "2025-01-01T12:00:00.000000",
    "trigger_source": "api_gateway",
    "request_id": "abc12345-def6-7890-ghij-klmnopqrstuv",
    "subreddits": ["LawFirm", "Lawyertalk", "legaltech"],
    "crawl_type": "both",
    "days_back": 3
  }
}
```

### 3. GET /status/{executionName} - Check Job Status
Check the status of a specific crawl job.

**Response:**
```json
{
  "executionArn": "arn:aws:states:us-east-1:123456789:execution:supio-reddit-insights-pipeline:manual-20250101-120000-abc12345",
  "status": "SUCCEEDED",
  "startDate": "2025-01-01T12:00:00.000000",
  "stopDate": "2025-01-01T12:15:30.000000",
  "input": {
    "manual_trigger": true,
    "trigger_time": "2025-01-01T12:00:00.000000",
    "subreddits": ["LawFirm", "Lawyertalk", "legaltech"]
  },
  "output": {
    "statusCode": 200,
    "posts_collected": 45,
    "comments_collected": 123,
    "s3_location": "s3://bucket/reddit/2025-01-01/crawl_20250101_120000.json"
  }
}
```

**Status Values:**
- `RUNNING` - Job is currently executing
- `SUCCEEDED` - Job completed successfully
- `FAILED` - Job failed
- `TIMED_OUT` - Job exceeded timeout limit
- `ABORTED` - Job was manually stopped

### 4. GET /executions - List Recent Jobs
Get a list of the 10 most recent crawl job executions.

**Response:**
```json
{
  "executions": [
    {
      "executionArn": "arn:aws:states:us-east-1:123456789:execution:supio-reddit-insights-pipeline:manual-20250101-120000-abc12345",
      "name": "manual-20250101-120000-abc12345",
      "status": "SUCCEEDED",
      "startDate": "2025-01-01T12:00:00.000000",
      "stopDate": "2025-01-01T12:15:30.000000"
    }
  ],
  "count": 1
}
```

## Cloudflare Workers Integration Example

Here's how to integrate this API into your Cloudflare Workers:

```typescript
// types/crawler.ts
export interface CrawlTriggerRequest {
  subreddits?: string[];
  crawl_type?: 'crawl' | 'search' | 'both';
  days_back?: number;
}

export interface CrawlTriggerResponse {
  message: string;
  executionArn: string;
  executionName: string;
  startDate: string;
  parameters: any;
}

export interface CrawlStatusResponse {
  executionArn: string;
  status: 'RUNNING' | 'SUCCEEDED' | 'FAILED' | 'TIMED_OUT' | 'ABORTED';
  startDate: string;
  stopDate?: string;
  input: any;
  output?: any;
}

// services/crawlerApi.ts
export class CrawlerApiService {
  private baseUrl: string;
  private apiKey: string;

  constructor() {
    this.baseUrl = process.env.CRAWLER_API_URL || '';
    this.apiKey = process.env.CRAWLER_API_KEY || '';
  }

  private async request(endpoint: string, options: RequestInit = {}): Promise<Response> {
    const url = `${this.baseUrl}${endpoint}`;

    const headers = {
      'Content-Type': 'application/json',
      'X-API-Key': this.apiKey,
      ...options.headers,
    };

    return fetch(url, {
      ...options,
      headers,
    });
  }

  async triggerCrawl(params?: CrawlTriggerRequest): Promise<CrawlTriggerResponse> {
    const response = await this.request('/trigger', {
      method: 'POST',
      body: params ? JSON.stringify(params) : undefined,
    });

    if (!response.ok) {
      throw new Error(`Failed to trigger crawl: ${response.statusText}`);
    }

    return response.json();
  }

  async getJobStatus(executionName: string): Promise<CrawlStatusResponse> {
    const response = await this.request(`/status/${executionName}`);

    if (!response.ok) {
      if (response.status === 404) {
        throw new Error('Job not found');
      }
      throw new Error(`Failed to get job status: ${response.statusText}`);
    }

    return response.json();
  }

  async listRecentJobs(): Promise<{ executions: any[]; count: number }> {
    const response = await this.request('/executions');

    if (!response.ok) {
      throw new Error(`Failed to list jobs: ${response.statusText}`);
    }

    return response.json();
  }
}

// Usage in Cloudflare Workers route handler
export default {
  async fetch(request: Request, env: Env, ctx: ExecutionContext): Promise<Response> {
    const crawlerApi = new CrawlerApiService();

    if (request.method === 'POST' && new URL(request.url).pathname === '/api/crawler/trigger') {
      try {
        const body = await request.json() as CrawlTriggerRequest;
        const result = await crawlerApi.triggerCrawl(body);

        return Response.json({
          success: true,
          data: result
        });
      } catch (error) {
        return Response.json({
          success: false,
          error: error.message
        }, { status: 500 });
      }
    }

    if (request.method === 'GET' && new URL(request.url).pathname.startsWith('/api/crawler/status/')) {
      try {
        const executionName = new URL(request.url).pathname.split('/').pop();
        const result = await crawlerApi.getJobStatus(executionName!);

        return Response.json({
          success: true,
          data: result
        });
      } catch (error) {
        return Response.json({
          success: false,
          error: error.message
        }, { status: error.message === 'Job not found' ? 404 : 500 });
      }
    }

    return new Response('Not found', { status: 404 });
  }
};
```

## Environment Variables

Set these environment variables in your Cloudflare Workers:

```bash
CRAWLER_API_URL=https://your-api-id.execute-api.us-east-1.amazonaws.com/v1
CRAWLER_API_KEY=your-api-key-here
```

## Error Handling

The API returns standard HTTP status codes:

- `200` - Success
- `400` - Bad Request (invalid parameters)
- `404` - Not Found (invalid endpoint or execution not found)
- `429` - Too Many Requests (rate limit exceeded)
- `500` - Internal Server Error

All error responses include a JSON body with error details:
```json
{
  "error": "Error message here",
  "message": "Additional details if available"
}
```

## Rate Limits

The API is configured with the following limits:
- **Rate Limit**: 10 requests per second
- **Burst Limit**: 20 concurrent requests
- **Monthly Quota**: 1000 requests per month

These limits can be adjusted in the CDK configuration if needed.

## CORS Support

The API includes full CORS support with:
- `Access-Control-Allow-Origin: *`
- `Access-Control-Allow-Methods: GET,POST,OPTIONS`
- `Access-Control-Allow-Headers: Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token`

## OpenAPI Schema

### Complete OpenAPI 3.0 Specification

```yaml
openapi: 3.0.3
info:
  title: Supio Reddit Legal Communities Crawler API
  description: |
    REST API for triggering and monitoring Reddit crawl jobs that collect legal technology
    feedback from Reddit communities, analyze sentiment using AI, and store actionable insights.

    This API serves as the backend for Cloudflare Workers and frontend applications to:
    - Trigger manual Reddit data collection jobs
    - Monitor execution status and progress
    - Access collected insights and analytics
    - Manage crawl job configurations
  version: 1.0.0
  contact:
    name: Supio Engineering Team
    email: engineering@supio.com
  license:
    name: MIT
    url: https://opensource.org/licenses/MIT

servers:
  - url: https://6bsn9muwfi.execute-api.us-west-2.amazonaws.com/v1
    description: Production API Gateway

security:
  - ApiKeyAuth: []

paths:
  /:
    get:
      summary: Get API Documentation
      description: Returns comprehensive API documentation and available endpoints
      operationId: getApiDocumentation
      tags:
        - Documentation
      responses:
        '200':
          description: API documentation retrieved successfully
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ApiDocumentation'
        '500':
          $ref: '#/components/responses/InternalServerError'

  /trigger:
    post:
      summary: Trigger New Crawl Job
      description: |
        Start a new Reddit crawl job with optional parameters.
        The job will collect posts from specified subreddits, analyze them with AI,
        and store actionable insights in DynamoDB.
      operationId: triggerCrawlJob
      tags:
        - Job Management
      requestBody:
        description: Optional crawl parameters to customize the job
        required: false
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CrawlTriggerRequest'
            examples:
              default:
                summary: Default crawl (all subreddits, 3 days back)
                value: {}
              custom:
                summary: Custom crawl parameters
                value:
                  subreddits: ["LawFirm", "Lawyertalk", "legaltech"]
                  crawl_type: "both"
                  days_back: 2
                  min_score: 8
              quick_test:
                summary: Quick test crawl
                value:
                  days_back: 1
                  min_score: 5
                  crawl_type: "search"
      responses:
        '200':
          description: Crawl job started successfully
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CrawlTriggerResponse'
        '400':
          $ref: '#/components/responses/BadRequest'
        '409':
          description: A crawl job with this name is already running
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ErrorResponse'
        '500':
          $ref: '#/components/responses/InternalServerError'

  /status/{executionName}:
    get:
      summary: Get Job Status
      description: |
        Check the status of a specific crawl job execution.
        Returns detailed information including current status, timestamps,
        input parameters, and output results.
      operationId: getJobStatus
      tags:
        - Job Management
      parameters:
        - name: executionName
          in: path
          required: true
          description: The unique execution name returned when triggering a job
          schema:
            type: string
            pattern: '^manual-[0-9]{8}-[0-9]{6}-[a-f0-9]{8}$'
            example: manual-20250923-021621-e5cff012
      responses:
        '200':
          description: Job status retrieved successfully
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/JobStatusResponse'
        '404':
          description: Job execution not found
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ErrorResponse'
        '500':
          $ref: '#/components/responses/InternalServerError'

  /executions:
    get:
      summary: List Recent Executions
      description: |
        Get a list of the 10 most recent crawl job executions with basic status information.
        Useful for building execution history dashboards and monitoring overall system health.
      operationId: listRecentExecutions
      tags:
        - Job Management
      responses:
        '200':
          description: Execution list retrieved successfully
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ExecutionListResponse'
        '500':
          $ref: '#/components/responses/InternalServerError'

components:
  securitySchemes:
    ApiKeyAuth:
      type: apiKey
      in: header
      name: X-API-Key
      description: API key for authentication (required for all endpoints)

  schemas:
    CrawlTriggerRequest:
      type: object
      description: Optional parameters for customizing crawl job execution
      properties:
        subreddits:
          type: array
          items:
            type: string
          description: List of subreddit names to crawl (without r/ prefix)
          example: ["LawFirm", "Lawyertalk", "legaltech", "legaltechAI"]
          maxItems: 10
        crawl_type:
          type: string
          enum: [crawl, search, both]
          description: |
            Type of crawl to perform:
            - crawl: Get recent posts from subreddit listings
            - search: Search for specific keywords
            - both: Perform both crawl and search (default)
          default: both
        days_back:
          type: integer
          minimum: 1
          maximum: 30
          description: Number of days to look back for posts
          default: 3
          example: 2
        min_score:
          type: integer
          minimum: 0
          description: Minimum post score (upvotes) threshold to filter noise
          default: 10
          example: 8

    CrawlTriggerResponse:
      type: object
      required:
        - message
        - executionArn
        - executionName
        - startDate
        - parameters
      properties:
        message:
          type: string
          example: "Crawl job started successfully"
        executionArn:
          type: string
          description: AWS Step Functions execution ARN for tracking
          example: "arn:aws:states:us-west-2:705247044519:execution:supio-reddit-insights-pipeline:manual-20250923-021621-e5cff012"
        executionName:
          type: string
          description: Unique execution name for status checking
          example: "manual-20250923-021621-e5cff012"
        startDate:
          type: string
          format: date-time
          description: ISO 8601 timestamp when the job started
          example: "2025-09-23T02:16:21.540000+00:00"
        parameters:
          type: object
          description: Actual parameters used for the crawl job
          properties:
            manual_trigger:
              type: boolean
              example: true
            trigger_time:
              type: string
              format: date-time
            trigger_source:
              type: string
              example: "api_gateway"
            request_id:
              type: string
              example: "e5cff012-c842-418a-a754-11238ea18615"
            subreddits:
              type: array
              items:
                type: string
            crawl_type:
              type: string
            days_back:
              type: integer
            min_score:
              type: integer

    JobStatusResponse:
      type: object
      required:
        - executionArn
        - status
        - startDate
        - input
      properties:
        executionArn:
          type: string
          example: "arn:aws:states:us-west-2:705247044519:execution:supio-reddit-insights-pipeline:manual-20250923-021621-e5cff012"
        status:
          type: string
          enum: [RUNNING, SUCCEEDED, FAILED, TIMED_OUT, ABORTED]
          description: Current execution status
          example: "SUCCEEDED"
        startDate:
          type: string
          format: date-time
          example: "2025-09-23T02:16:21.540000+00:00"
        stopDate:
          type: string
          format: date-time
          nullable: true
          description: When the job completed (null if still running)
          example: "2025-09-23T02:19:48.610000+00:00"
        input:
          type: object
          description: Input parameters provided when the job was triggered
        output:
          type: object
          nullable: true
          description: Job results (null if still running or failed)
          properties:
            statusCode:
              type: integer
              example: 200
            insights_stored:
              type: integer
              description: Number of insights stored in DynamoDB
            high_priority_count:
              type: integer
              description: Number of high priority insights (score >= 8)
            alerts:
              type: array
              items:
                type: object
                properties:
                  post_id:
                    type: string
                  priority:
                    type: integer
                  summary:
                    type: string
                  action:
                    type: string
            timestamp:
              type: string
              format: date-time

    ExecutionListResponse:
      type: object
      required:
        - executions
        - count
      properties:
        executions:
          type: array
          items:
            $ref: '#/components/schemas/ExecutionSummary'
          maxItems: 10
        count:
          type: integer
          description: Number of executions returned
          example: 7

    ExecutionSummary:
      type: object
      required:
        - executionArn
        - name
        - status
        - startDate
      properties:
        executionArn:
          type: string
        name:
          type: string
          example: "manual-20250923-021621-e5cff012"
        status:
          type: string
          enum: [RUNNING, SUCCEEDED, FAILED, TIMED_OUT, ABORTED]
        startDate:
          type: string
          format: date-time
        stopDate:
          type: string
          format: date-time
          nullable: true

    ApiDocumentation:
      type: object
      properties:
        service:
          type: string
          example: "Supio Reddit Legal Communities Crawler API"
        version:
          type: string
          example: "1.0"
        description:
          type: string
        endpoints:
          type: object
        status_values:
          type: array
          items:
            type: string
        authentication:
          type: string
        cors:
          type: string

    ErrorResponse:
      type: object
      required:
        - error
      properties:
        error:
          type: string
          description: Error message describing what went wrong
          example: "Invalid JSON in request body"
        message:
          type: string
          description: Additional error details if available
          example: "days_back must be a positive integer"

  responses:
    BadRequest:
      description: Invalid request parameters
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/ErrorResponse'
          examples:
            invalid_json:
              summary: Invalid JSON in request body
              value:
                error: "Invalid JSON in request body"
            invalid_crawl_type:
              summary: Invalid crawl_type parameter
              value:
                error: "crawl_type must be crawl, search, or both"
            invalid_days_back:
              summary: Invalid days_back parameter
              value:
                error: "days_back must be a positive integer"

    InternalServerError:
      description: Internal server error
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/ErrorResponse'
          example:
            error: "Internal server error"
            message: "Failed to start crawl job"

tags:
  - name: Documentation
    description: API documentation and metadata
  - name: Job Management
    description: Operations for managing crawl jobs and monitoring execution
```

## Deployment

To deploy the API:

1. Ensure Reddit API credentials are configured
2. Deploy the CDK stack:
   ```bash
   npx cdk deploy --context redditClientId=YOUR_CLIENT_ID --context redditClientSecret=YOUR_CLIENT_SECRET
   ```
3. Note the API URL and API Key from the deployment outputs
4. Configure your Cloudflare Workers with the API URL and key

The API will be immediately available for triggering crawl jobs and monitoring their status.

## Missing RESTful Endpoints for Frontend Development

Based on the analysis of the current API and the data structures available in DynamoDB, here are the **critical missing endpoints** needed for comprehensive frontend development and long-term maintenance:

### üéØ **Priority 1: Essential Data Access Endpoints**

#### 1. **GET /insights** - List Insights with Filtering
```yaml
GET /insights?priority_min=5&category=document_automation&limit=50&date_from=2025-09-01
```
**Purpose**: Essential for dashboard views, feature prioritization, and product management workflows.

**Current Gap**: No way to access the collected insights stored in DynamoDB.

**Frontend Use Cases**:
- PM dashboard showing high-priority feature requests
- Feature category analysis and trending
- Competitive intelligence reports
- User segment analysis

#### 2. **GET /insights/{insightId}** - Get Single Insight Details
```yaml
GET /insights/INSIGHT#2025-09-22_PRIORITY#8#ID#abc123
```
**Purpose**: Detailed view of specific insights with full Reddit post context.

**Frontend Use Cases**:
- Detailed insight modal/page
- Direct link sharing for PM reviews
- Full post content and analysis viewing

#### 3. **GET /analytics/summary** - Analytics Dashboard Data
```yaml
GET /analytics/summary?period=7d&group_by=category,user_segment
```
**Purpose**: Aggregated metrics for executive dashboards and reporting.

**Frontend Use Cases**:
- Executive summary dashboards
- Weekly/monthly reports
- Trend analysis charts
- KPI tracking

### üîß **Priority 2: Configuration & Management Endpoints**

#### 4. **GET /config** & **PUT /config** - Crawl Configuration Management
```yaml
GET /config
PUT /config
```
**Purpose**: Manage default crawl parameters, subreddit lists, and system settings.

**Current Gap**: No centralized config management - parameters are hardcoded.

**Frontend Use Cases**:
- Admin panel for system configuration
- Subreddit management interface
- Crawl parameter tuning
- Feature flag management

#### 5. **GET /health** - System Health & Metrics
```yaml
GET /health
```
**Purpose**: System monitoring, uptime tracking, and operational insights.

**Frontend Use Cases**:
- System status dashboard
- Operational monitoring
- Error rate tracking
- Performance metrics

### üìä **Priority 3: Enhanced Analytics Endpoints**

#### 6. **GET /analytics/trends** - Trend Analysis
```yaml
GET /analytics/trends?metric=priority_score&period=30d&group_by=week
```
**Purpose**: Historical trend analysis for strategic planning.

**Frontend Use Cases**:
- Trend visualization charts
- Market demand analysis
- Seasonal pattern identification
- Forecasting dashboards

#### 7. **GET /analytics/competitors** - Competitive Intelligence
```yaml
GET /analytics/competitors?competitor=Harvey&sentiment=negative&limit=20
```
**Purpose**: Competitive analysis and market positioning insights.

**Frontend Use Cases**:
- Competitor monitoring dashboards
- Market positioning analysis
- Sentiment tracking
- Competitive advantage identification

### üõ†Ô∏è **Priority 4: Operational Endpoints**

#### 8. **DELETE /executions/{executionName}** - Cancel Running Jobs
```yaml
DELETE /executions/manual-20250923-021621-e5cff012
```
**Purpose**: Operational control for stopping runaway or erroneous jobs.

**Frontend Use Cases**:
- Emergency job termination
- Resource management
- Cost control
- Operational oversight

#### 9. **GET /logs/{executionName}** - Execution Logs
```yaml
GET /logs/manual-20250923-021621-e5cff012?level=error&limit=100
```
**Purpose**: Debugging, troubleshooting, and operational visibility.

**Frontend Use Cases**:
- Debug interfaces
- Error investigation
- Performance analysis
- Audit trails

### üìã **Implementation Recommendations**

#### **Phase 1: Immediate (Next Sprint)**
1. **GET /insights** - Critical for PM workflows
2. **GET /insights/{insightId}** - Essential for detailed views
3. **GET /health** - Basic operational monitoring

#### **Phase 2: Short-term (1-2 Months)**
4. **GET /analytics/summary** - Dashboard requirements
5. **GET /config** & **PUT /config** - Configuration management
6. **DELETE /executions/{executionName}** - Operational control

#### **Phase 3: Medium-term (3-6 Months)**
7. **GET /analytics/trends** - Strategic analytics
8. **GET /analytics/competitors** - Competitive intelligence
9. **GET /logs/{executionName}** - Advanced debugging

### üèóÔ∏è **Implementation Architecture**

#### **DynamoDB Integration**
- Add new Lambda function: `/infra/aws/lambda/insights-api/`
- Implement DynamoDB query patterns for GSI1 access
- Add CloudWatch integration for analytics endpoints

#### **CDK Infrastructure Updates**
```typescript
// Add DynamoDB read permissions for insights API
insightsTable.grantReadData(insightsApiFunction);

// Add new API Gateway resources
const insightsResource = api.root.addResource('insights');
const analyticsResource = api.root.addResource('analytics');
const configResource = api.root.addResource('config');
```

#### **Response Standardization**
```typescript
interface StandardApiResponse<T> {
  data: T;
  pagination?: {
    page: number;
    limit: number;
    total: number;
    hasNext: boolean;
  };
  meta?: {
    timestamp: string;
    version: string;
    requestId: string;
  };
}
```

### üöÄ **Long-term Maintenance Benefits**

1. **Product Management Efficiency**: Direct access to insights without manual database queries
2. **Operational Visibility**: Full system observability and control
3. **Scalability**: RESTful patterns support frontend framework integration
4. **Developer Experience**: Comprehensive OpenAPI schema enables auto-generated client SDKs
5. **Business Intelligence**: Rich analytics endpoints support strategic decision-making

### üìä **ROI Justification**

- **Time Savings**: Reduce PM manual work by 70% with direct insight access
- **Decision Speed**: Enable real-time feature prioritization with live data
- **Operational Efficiency**: Reduce debugging time with proper logging endpoints
- **Strategic Value**: Enable data-driven product decisions with trend analysis

The current 4-endpoint API covers **job management** well, but is missing **80% of the data access patterns** needed for a production product management tool. Implementing these missing endpoints transforms the system from a basic crawler to a comprehensive product intelligence platform.